{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM,Dense,Flatten\n",
    "from keras.preprocessing.text import *\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.utils import to_categorical\n",
    "from pickle import load\n",
    "from pickle import dump\n",
    "from keras.models import load_model\n",
    "import string\n",
    "from numpy import array\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('path_to_\\Bible_for_word.txt','r')\n",
    "lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 1, 669, 26, 1268, 1, 171, 2, 1, 109, 2, 1, 109, 27, 222, 1972, 2, 1973, 2, 482, 27, 44, 1, 227, 3, 1, 956, 2, 1, 187, 3, 26, 852, 44, 1, 227, 3, 1, 303]\n",
      "[2, 26, 36, 79, 53, 16, 320, 2, 53, 27, 320]\n",
      "[  3   1 956   2   1 187   3  26 852  44   1 227   3   1 303]\n",
      "[  2  26  36  79  53  16 320   2  53  27 320   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)\n",
    "print(sequences[0])\n",
    "print(sequences[1])\n",
    "sequences = pad_sequences(sequences, maxlen=15, padding='post')\n",
    "print(sequences[0])\n",
    "print(sequences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12606\n"
     ]
    }
   ],
   "source": [
    "# vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30382, 15)\n",
      "[  3   1 956   2   1 187   3  26 852  44   1 227   3   1]\n",
      "[303]\n",
      "(30382, 14)\n",
      "(30382, 12606)\n"
     ]
    }
   ],
   "source": [
    "# separate into input and output\n",
    "sequences = np.array(sequences)\n",
    "print(sequences.shape)\n",
    "X, y = np.array(sequences[:,:-1]), np.array(sequences[:,-1])\n",
    "print(X[0])\n",
    "y = y.reshape(30382,1)\n",
    "print(y[0])\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = X.shape[1]\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 14, 100)           1260600   \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 14, 100)           80400     \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 12606)             1273206   \n",
      "=================================================================\n",
      "Total params: 2,704,706\n",
      "Trainable params: 2,704,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, input_length=seq_length))\n",
    "model.add(LSTM(100, return_sequences=True)) \n",
    "model.add(LSTM(100)) \n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit model\n",
    "model.fit(X, y, batch_size=128, epochs=50)\n",
    "\n",
    "# save the model to file\n",
    "model.save('path_to_\\model.h5')\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('path_to_\\tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the earth bringeth forth fruit of herself first the blade then the ear after that the full corn in the ear \n",
      "\n",
      "side them out away up it up it up it up it out it out it up it out it out it up it out it out it up it out it out it up it out it out it up it out it out it up it out it\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    " \n",
    "# generate a sequence from a language model\n",
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "    result = list()\n",
    "    in_text = seed_text\n",
    "    # generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        # encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        # predict probabilities for each word\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "        # append to input\n",
    "        in_text += ' ' + out_word\n",
    "        result.append(out_word)\n",
    "    return ' '.join(result)\n",
    " \n",
    "# load cleaned text sequences\n",
    "in_filename = 'path_to_\\Bible_for_word.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "#seq_length = len(lines[0].split()) - 1\n",
    "seq_length = 14 \n",
    "# load the model\n",
    "model = load_model('path_to_\\model.h5')\n",
    " \n",
    "# load the tokenizer\n",
    "tokenizer = load(open('path_to_\\tokenizer.pkl', 'rb'))\n",
    " \n",
    "# select a seed text\n",
    "seed_text = lines[randint(0,len(lines))]\n",
    "print(seed_text + '\\n')\n",
    " \n",
    "# generate new text\n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
    "print(generated) # the output below is due to 50 epochs and accuracy = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
